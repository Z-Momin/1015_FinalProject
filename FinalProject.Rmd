<<<<<<< HEAD
---
title: "FinalProject"
author: "Zafir Momin"
date: "4/24/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir ="C:\\Users\\zafir\\Documents\\Text as Data\\Project\\1015_FinalProject")
```

```{r echo=FALSE}
# import libraries
# rm(list=ls())

library(quanteda)
library(dplyr)
library(readtext)
library(quanteda.textmodels)
library(stringi)
library(tm)
library(wordcloud)
# Change to directory with csv
# setwd("C:\\Users\\zafir\\Documents\\Text as Data\\Project\\eui-text-workshop\\datasets")
```


```{r echo=TRUE}
tweets <- read.csv("candidate-tweets.csv", stringsAsFactors=F)
tweets$text <- gsub('https?://t.co/[A-Za-z0-9]+', '', tweets$text)
# tweets$candidate <- tweets$screen_name
tweets$candidate <- factor(tweets$screen_name, levels = unique(tweets$screen_name), labels = c(1, 2, 3, 4))

tweets <- tweets %>%  select(datetime, text, screen_name, candidate)
tweets$retweet <- 0
tweets$retweet[grep('RT @',tweets$text)] <- 1

tweets$datetime = mdy(paste(month(tweets$datetime), day(tweets$datetime), year(tweets$datetime), sep = "-")) # date only

tweets_NoRT <- tweets[grep(0, tweets$retweet),]
```

```{r echo=TRUE}
t_corpus <- corpus(tweets$text)

flesch_scores <- data.frame(textstat_readability(t_corpus, "Flesch.Kincaid"))

flesch_scores$screen_name <- tweets$screen_name

flesch_scores %>% group_by(screen_name) %>% summarise(Avg = mean(Flesch.Kincaid,na.rm=TRUE))
```



```{r echo=TRUE}
t_corpus <- corpus(tweets_NoRT$text)

flesch_scores <- data.frame(textstat_readability(t_corpus, "Flesch.Kincaid"))

flesch_scores$screen_name <- tweets_NoRT$screen_name

flesch_scores %>% group_by(screen_name) %>% summarise(Avg = mean(Flesch.Kincaid,na.rm=TRUE))

```

```{r echo=TRUE}
filenames <- list.files("Speeches")
party <- unlist(regmatches(unlist(filenames), gregexpr("^[[:alpha:]]{3}", unlist(filenames))))
year <- unlist(regmatches(unlist(filenames), gregexpr("[[:digit:]]+", unlist(filenames))))
candidate <- unlist(substr( regmatches(unlist(filenames), gregexpr("^[[:alpha:]]{7}", unlist(filenames))), 4, 7))

n_conv_speeches <- corpus(readtext("Speeches/*.txt", encoding="UTF-8"))
candidate_flesh_df <- data.frame(textstat_readability(n_conv_speeches, "Flesch.Kincaid"))
candidate_flesh_df <- cbind(candidate_flesh_df, party, candidate, year)
candidate_flesh_df

# flesch_scores %>% group_by(candidate) %>% summarise(Avg = mean(Dickes.Steiwer,na.rm=TRUE))

```

```{r echo=TRUE}
filenames <- list.files("Debates")
party <- unlist(regmatches(unlist(filenames), gregexpr("^[[:alpha:]]{3}", unlist(filenames))))
year <- unlist(regmatches(unlist(filenames), gregexpr("[[:digit:]]+", unlist(filenames))))
candidate <- unlist(substr( regmatches(unlist(filenames), gregexpr("^[[:alpha:]]{7}", unlist(filenames))), 4, 7))

debate_corpus <- corpus(readtext("Debates/*.txt", encoding="UTF-8"))
debate_flesh_df <- data.frame(textstat_readability(debate_corpus, "Flesch.Kincaid"))
debate_flesh_df <- cbind(debate_flesh_df, party, candidate, year)
debate_flesh_df

# flesch_scores %>% group_by(candidate) %>% summarise(Avg = mean(Dickes.Steiwer,na.rm=TRUE))

```


WordScores Model
```{r echo=TRUE}
filenames <- list.files("Speeches")

# Party name and year are in the filename -- we can use regex to extract these to use as our docvars
party <- unlist(regmatches(unlist(filenames), gregexpr("^[[:alpha:]]{3}", unlist(filenames))))
candidate <- unlist(substr( regmatches(unlist(filenames), gregexpr("^[[:alpha:]]{7}", unlist(filenames))), 4, 7))

# Make a corpus with docvars from this data
n_conv_speeches <- corpus(readtext("Speeches/*.txt", encoding="UTF-8"))
docvars(n_conv_speeches, field = c("party", "candidate") ) <- data.frame(cbind(party, candidate))

# But we're going to use a dataframe
dem_rep_df <- tibble(text = texts(n_conv_speeches),
                         class = party,
                         candidate = candidate)

set.seed(100L)
ids <- 1:nrow(dem_rep_df)
ids_test <- sample(ids, 1, replace = FALSE) #we are just excluding one for the test set
ids_train <- ids[-ids_test]
train_set <- dem_rep_df[ids_train,]
test_set <- dem_rep_df[ids_test,]

# create DFMs
train_dfm <- dfm(train_set$text, remove_punct = TRUE, remove = stopwords("english"))
test_dfm <- dfm(test_set$text, remove_punct = TRUE, remove = stopwords("english"))

ws_sm <- textmodel_wordscores(train_dfm, 
                              y = (2 * as.numeric(train_set$class == "Dem")) - 1,
                              smooth = 1
)

# Look at strongest features
dem_features_sm <- sort(ws_sm$wordscores, decreasing = TRUE)  # for labor
dem_features_sm[1:10]

rep_features_sm <- sort(ws_sm$wordscores, decreasing = FALSE)  # for conservative
rep_features_sm[1:10]
```



############################ Val ##################################


```{r echo=TRUE}
tweets <- read.csv("candidate-tweets.csv", stringsAsFactors=F)
tweets$text <- gsub('https?://t.co/[A-Za-z0-9]+', '', tweets$text)
# tweets$candidate <- tweets$screen_name
tweets$candidate <- factor(tweets$screen_name, levels = unique(tweets$screen_name), labels = c(1, 2, 3, 4))

tweets <- tweets %>%  select(datetime, text, screen_name, candidate)
tweets$retweet <- 0
tweets$retweet[grep('RT @',tweets$text)] <- 1
tweets$text <- gsub("amp", "", tweets$text) 
tweets$text <- stemDocument(tweets$text) 
tweets$text = removeWords(tweets$text, stopwords("english"))
tweets$text = stripWhitespace(tweets$text)
library(lubridate)
tweets$datetime = mdy(paste(month(tweets$datetime), day(tweets$datetime), year(tweets$datetime), sep = "-")) # date only

tweets_NoRT <- tweets[grep(0, tweets$retweet),]
```

```{r echo=TRUE}

t_corpus <- corpus(tweets$text)

#t_corpus <- tolower(t_corpus)

flesch_scores <- data.frame(textstat_readability(t_corpus, "Flesch"))

flesch_scores$screen_name <- tweets$screen_name

flesch_scores %>% group_by(screen_name) %>% summarise(Avg = mean(Flesch,na.rm=TRUE))

```

```{r echo=TRUE}

t_corpus <- corpus(tweets_NoRT$text)

flesch_scores <- data.frame(textstat_readability(t_corpus, "Flesch"))

flesch_scores$screen_name <- tweets_NoRT$screen_name

flesch_scores %>% group_by(screen_name) %>% summarise(Avg = mean(Flesch,na.rm=TRUE))

```

```{r}
Trump_tweet <- ''
Trump <- tweets_NoRT[tweets_NoRT$candidate==1,]
for (i in 1:nrow(Trump)){
  Trump_tweet <- paste(Trump_tweet,Trump[i,2])
}
Clinton_tweet <- ''
Clinton <- tweets_NoRT[tweets_NoRT$candidate==2,]
for (i in 1:nrow(Clinton)){
  Clinton_tweet <- paste(Clinton_tweet,Clinton[i,2])
}
Cruz_tweet <- ''
Cruz <- tweets_NoRT[tweets_NoRT$candidate==3,]
for (i in 1:nrow(Cruz)){
  Cruz_tweet <- paste(Cruz_tweet,Cruz[i,2])
}
Sanders_tweet <- ''
Sanders <- tweets_NoRT[tweets_NoRT$candidate==4,]
for (i in 1:nrow(Cruz)){
  Sanders_tweet <- paste(Sanders_tweet,Sanders[i,2])
}

candidate_dfm <- dfm(c(Trump_tweet, Clinton_tweet,Cruz_tweet,Sanders_tweet))
similarity <- textstat_simil(candidate_dfm, margin = "documents", method = "cosine")
rownames(similarity) <- c('Trump','Clinton','Cruz','Sanders')
colnames(similarity) <- c('Trump','Clinton','Cruz','Sanders')
as.matrix(similarity)
```

```{r}
dtm <- TermDocumentMatrix(Corpus(VectorSource(c(Clinton_tweet,Sanders_tweet))))
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=70, random.order=FALSE, rot.per=0.1,            colors=brewer.pal(8, "Dark2"))
```

```{r}
dtm <- TermDocumentMatrix(Corpus(VectorSource(c(Cruz_tweet,Trump_tweet))))
matrix <- as.matrix(dtm) 
words <- sort(rowSums(matrix),decreasing=TRUE) 
df <- data.frame(word = names(words),freq=words)
wordcloud(words = df$word, freq = df$freq, min.freq = 1,           max.words=70, random.order=FALSE, rot.per=0.1,            colors=brewer.pal(8, "Dark2"))
```

# burstiness
```{r}
library(bursts)
library(zoo)

bursty <- function(word, DTM, date) {
  word.vec <- DTM[, which(colnames(DTM) == word)]
  if(length(word.vec) == 0) {
    print(paste(word, " does not exist in this corpus."))
    return()
  } 
  else {
    word.times <- c(0,which(as.vector(word.vec)>0))
    
    kl <- kleinberg(word.times, gamma = 0.5)
    kl$start <- date[kl$start+1]
    kl$end <- date[kl$end]
    max_level <- max(kl$level)
    
    plot(c(kl$start[1], kl$end[1]), c(1,max_level),
         type = "n", ylab = "Level", bty = "n",
         xlim = c(min(date),max(date)),ylim = c(1, max_level),xlab = '',
         yaxt = "n")
    axis(2, at = 1:max_level)
    axis(1, date, as.Date(date), cex.axis = 1,line = 1.28)    
    
    for (i in 1:nrow(kl)) {
      if (kl$start[i] != kl$end[i]) {
        arrows(kl$start[i], kl$level[i], kl$end[i], kl$level[i], code = 3, angle = 90,
               length = 0.05)
      } 
      else {
        points(kl$start[i], kl$level[i])
      }
    }
    
    print(kl)
  }
  #note deviation from standard defaults bec don't have that much data
}

library(lubridate)
Trump_corpus <- corpus(Trump,text_field = 'text')
Clinton_corpus <- corpus(Clinton,text_field = 'text')
Cruz_corpus <- corpus(Cruz,text_field = 'text')
Sanders_corpus <- corpus(Sanders,text_field = 'text')

news_dfm <- dfm(news_data)

Trump_dfm <- dfm(Trump_corpus)
Clinton_dfm <- dfm(Clinton_corpus)
Cruz_dfm <- dfm(Cruz_corpus)
Sanders_dfm <- dfm(Sanders_corpus)

bursty("immigration", Trump_dfm, docvars(Trump_corpus)$date)
bursty("immigrant", Trump_dfm, docvars(Trump_corpus)$date)
bursty("obama", Trump_dfm, docvars(Trump_corpus)$date)
bursty("immigr", Clinton_dfm, docvars(Clinton_corpus)$date)
```

```{r}
bursty <- function(word, DTM, date) {
  word.vec <- DTM[, which(colnames(DTM) == word)]
  if(length(word.vec) == 0) {
    print(paste(word, " does not exist in this corpus."))
    return()
  } 
  else {
    word.times <- c(0,which(as.vector(word.vec)>0))
    
    kl <- kleinberg(word.times, gamma = 0.5)
    kl$start <- date[kl$start+1]
    kl$end <- date[kl$end]
    max_level <- max(kl$level)
    
    plot(c(kl$start[1], kl$end[1]), c(1,max_level),
         type = "n", xlab = "Time", ylab = "Level", bty = "n",
         xlim = c(min(date), max(date)), ylim = c(1, max_level),
         yaxt = "n")
    axis(2, at = 1:max_level)
    
    for (i in 1:nrow(kl)) {
      if (kl$start[i] != kl$end[i]) {
        arrows(kl$start[i], kl$level[i], kl$end[i], kl$level[i], code = 3, angle = 90,
               length = 0.05)
      } 
      else {
        points(kl$start[i], kl$level[i])
      }
    }
    
    print(kl)
  }
  #note deviation from standard defaults bec don't have that much data
}

bursty("immigr", Clinton_dfm, year(docvars(Clinton_corpus)$date))


```
















